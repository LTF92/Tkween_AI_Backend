{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Arabic Poems Dataset Preparation\n",
        "This notebook loads the `arbml/ashaar` dataset from Hugging Face and creates embeddings using OpenAI's `text-embedding-3-small` model, then stores them in ChromaDB.\n",
        "\n",
        "## Features:\n",
        "- Resume from last checkpoint (RESUME_FROM_LAST)\n",
        "- Batch processing for efficiency\n",
        "- Text truncation for long poems (MAX_TEXT_LENGTH)\n",
        "- Metadata storage (poet, meter, theme, era, location)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install datasets openai chromadb tqdm python-dotenv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "import chromadb\n",
        "from tqdm import tqdm\n",
        "from dotenv import load_dotenv\n",
        "import time\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "BATCH_SIZE = 100\n",
        "MAX_TEXT_LENGTH = 6000  # Maximum text length in characters\n",
        "RESUME_FROM_LAST = True  # Resume from last checkpoint\n",
        "\n",
        "# Initialize OpenAI client\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "print(\"‚úÖ Configuration loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the arbml/ashaar dataset from Hugging Face\n",
        "print(\"üìö Loading arbml/ashaar dataset...\")\n",
        "dataset = load_dataset(\"arbml/ashaar\", split=\"train\")\n",
        "print(f\"‚úÖ Dataset loaded successfully\")\n",
        "print(f\"üìä Total poems: {len(dataset):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore dataset structure\n",
        "print(\"Dataset columns:\")\n",
        "print(dataset.column_names)\n",
        "print(\"\\nFirst example:\")\n",
        "print(dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ChromaDB\n",
        "print(\"üóÑÔ∏è Initializing ChromaDB...\")\n",
        "chroma_client = chromadb.PersistentClient(path=\"./arabic_poems_db\")\n",
        "\n",
        "# Get or create collection\n",
        "collection = chroma_client.get_or_create_collection(\n",
        "    name=\"arabic_poems\",\n",
        "    metadata={\"description\": \"Arabic poems from arbml/ashaar dataset\"}\n",
        ")\n",
        "\n",
        "existing_count = collection.count()\n",
        "print(f\"‚úÖ ChromaDB initialized\")\n",
        "print(f\"üìä Existing poems in collection: {existing_count:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Get embedding for text using OpenAI's text-embedding-3-small model.\n",
        "    Truncates text if it exceeds MAX_TEXT_LENGTH.\n",
        "    \"\"\"\n",
        "    # Truncate text if too long\n",
        "    if len(text) > MAX_TEXT_LENGTH:\n",
        "        text = text[:MAX_TEXT_LENGTH]\n",
        "    \n",
        "    try:\n",
        "        response = openai_client.embeddings.create(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            input=text\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_verses(verses) -> str:\n",
        "    \"\"\"\n",
        "    Process verses - handles both string and list formats.\n",
        "    \"\"\"\n",
        "    if isinstance(verses, list):\n",
        "        return \"\\n\".join(verses)\n",
        "    return str(verses) if verses else \"\"\n",
        "\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine starting point\n",
        "start_idx = 0\n",
        "if RESUME_FROM_LAST and existing_count > 0:\n",
        "    start_idx = existing_count\n",
        "    print(f\"üîÑ Resuming from index {start_idx:,}\")\n",
        "else:\n",
        "    print(\"üÜï Starting from the beginning\")\n",
        "\n",
        "# Calculate batches\n",
        "total_poems = len(dataset)\n",
        "remaining = total_poems - start_idx\n",
        "num_batches = (remaining + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "print(f\"üìä Total poems: {total_poems:,}\")\n",
        "print(f\"üìä Remaining to process: {remaining:,}\")\n",
        "print(f\"üìä Number of batches: {num_batches:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process poems and create embeddings\n",
        "print(f\"\\nüîÑ Starting embeddings calculation and storing in ChromaDB...\")\n",
        "\n",
        "for batch_idx in tqdm(range(num_batches), desc=\"Processing poems\"):\n",
        "    batch_start = start_idx + (batch_idx * BATCH_SIZE)\n",
        "    batch_end = min(batch_start + BATCH_SIZE, total_poems)\n",
        "    \n",
        "    ids = []\n",
        "    documents = []\n",
        "    embeddings = []\n",
        "    metadatas = []\n",
        "    \n",
        "    for i in range(batch_start, batch_end):\n",
        "        poem = dataset[i]\n",
        "        \n",
        "        # Extract poem data - using arbml/ashaar column names\n",
        "        poem_id = f\"poem_{i}\"\n",
        "        poem_title = poem.get(\"poem title\", \"\")\n",
        "        poem_text = process_verses(poem.get(\"poem verses\", \"\"))\n",
        "        poet_name = poem.get(\"poet\", \"Unknown\")\n",
        "        poem_meter = poem.get(\"poem meter\", \"\")\n",
        "        poem_theme = poem.get(\"poem theme\", \"\")\n",
        "        poet_era = poem.get(\"poet era\", \"\")\n",
        "        poet_location = poem.get(\"poet location\", \"\")\n",
        "        \n",
        "        # Create full text for embedding\n",
        "        full_text = f\"{poem_title}\\n{poem_text}\"\n",
        "        \n",
        "        if not full_text.strip():\n",
        "            continue\n",
        "        \n",
        "        # Get embedding\n",
        "        embedding = get_embedding(full_text)\n",
        "        if embedding is None:\n",
        "            continue\n",
        "        \n",
        "        ids.append(poem_id)\n",
        "        documents.append(full_text[:MAX_TEXT_LENGTH])\n",
        "        embeddings.append(embedding)\n",
        "        metadatas.append({\n",
        "            \"poet\": poet_name,\n",
        "            \"title\": poem_title,\n",
        "            \"poem_meter\": poem_meter,\n",
        "            \"poem_theme\": poem_theme,\n",
        "            \"poet_era\": poet_era,\n",
        "            \"poet_location\": poet_location\n",
        "        })\n",
        "    \n",
        "    # Add batch to ChromaDB\n",
        "    if ids:\n",
        "        collection.add(\n",
        "            ids=ids,\n",
        "            documents=documents,\n",
        "            embeddings=embeddings,\n",
        "            metadatas=metadatas\n",
        "        )\n",
        "    \n",
        "    # Small delay to avoid rate limiting\n",
        "    time.sleep(0.1)\n",
        "\n",
        "print(f\"\\n‚úÖ Completed!\")\n",
        "print(f\"üìä Total poems stored: {collection.count():,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify the collection\n",
        "print(\"üîç Verifying collection...\")\n",
        "print(f\"üìä Total poems in collection: {collection.count():,}\")\n",
        "\n",
        "# Sample query\n",
        "sample = collection.peek(limit=3)\n",
        "print(\"\\nüìù Sample poems:\")\n",
        "for i, meta in enumerate(sample['metadatas']):\n",
        "    print(f\"  {i+1}. {meta.get('title', 'No title')} - {meta.get('poet', 'Unknown')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test semantic search\n",
        "print(\"üîç Testing semantic search...\")\n",
        "\n",
        "test_query = \"ÿßŸÑÿ≠ÿ® ŸàÿßŸÑÿ∫ÿ≤ŸÑ\"\n",
        "query_embedding = get_embedding(test_query)\n",
        "\n",
        "results = collection.query(\n",
        "    query_embeddings=[query_embedding],\n",
        "    n_results=5\n",
        ")\n",
        "\n",
        "print(f\"\\nSearch results for: '{test_query}'\")\n",
        "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
        "    print(f\"\\n{i+1}. {meta.get('title', 'No title')}\")\n",
        "    print(f\"   Poet: {meta.get('poet', 'Unknown')}\")\n",
        "    print(f\"   Meter: {meta.get('poem_meter', 'Unknown')}\")\n",
        "    print(f\"   Preview: {doc[:100]}...\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
